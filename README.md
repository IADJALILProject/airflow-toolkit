# airflow-toolkit

Librairie interne d‚Äô**operators**, **hooks** et **utilitaires** pour [Apache Airflow](https://airflow.apache.org/).

Objectif : factoriser le code r√©current (ingestion fichiers, HTTP, SQL, dbt, logging, config‚Ä¶) dans une
lib Python r√©utilisable entre plusieurs projets Airflow (data engineering, dataops, administration BD).

Version actuelle : **1.0.0**

---

## 1. Fonctionnalit√©s

### üß© Operators
- `FilesystemTransferOperator` ‚Üí copie de fichiers / dossiers, synchronisation.
- `HttpToFilesystemOperator` ‚Üí t√©l√©chargement HTTP(s) vers le syst√®me de fichiers.
- `DbtRunnerOperator` ‚Üí ex√©cution de commandes `dbt` orchestr√©es dans Airflow.
- `SqlExecuteOperator` ‚Üí ex√©cution de requ√™tes SQL g√©n√©riques (admin, maintenance, dataops).

### üîå Hooks
- `ClickHouseHook` ‚Üí connexion + ex√©cution de requ√™tes ClickHouse.

### üõ†Ô∏è Utilitaires
- `logging_utils.get_logger` ‚Üí logger structur√© et homog√®ne.
- `date_utils` ‚Üí helpers temporels (`utc_now`, `days_ago`).
- `env_utils.load_env_config` ‚Üí chargement YAML par environnement (`env_dev.yaml`, etc.).
- `alerts.slack_failure_alert` ‚Üí callback Slack en cas d‚Äô√©chec Airflow.

### üß™ Qualit√©
- Lint : `ruff`
- Formatage : `black`
- Tests : `pytest`
- CI : GitHub Actions (`ruff` + `black` + `pytest`)

---

## 2. Installation

### 2.1 Installation dans un autre projet Airflow

Dans `pyproject.toml` :

```toml
[project]
dependencies = [
  "apache-airflow>=2.9.0",
  "airflow-toolkit @ git+https://github.com/IADJALILProject/airflow-toolkit.git",
]
```

Ou installation directe :

```bash
pip install "git+https://github.com/IADJALILProject/airflow-toolkit.git"
```

---

## 3. Exemples d‚Äôutilisation

### 3.1 Import

```python
from airflow_toolkit.operators import (
    FilesystemTransferOperator,
    HttpToFilesystemOperator,
    DbtRunnerOperator,
    SqlExecuteOperator,
)
from airflow_toolkit.utils import get_logger, slack_failure_alert
```

---

### 3.2 DAG d‚Äôingestion fichiers

```python
from datetime import datetime
from airflow import DAG
from airflow_toolkit.operators import FilesystemTransferOperator

with DAG(
    dag_id="example_filesystem_ingestion",
    start_date=datetime(2025, 1, 1),
    schedule="@daily",
    catchup=False,
) as dag:
    transfer = FilesystemTransferOperator(
        task_id="copy_raw_to_processed",
        source_path="/data/raw",
        target_path="/data/processed",
    )
```

---

### 3.3 T√©l√©chargement HTTP ‚Üí filesystem

```python
from datetime import datetime
from airflow import DAG
from airflow_toolkit.operators import HttpToFilesystemOperator

with DAG(
    dag_id="example_http_download",
    start_date=datetime(2025, 1, 1),
    schedule="@daily",
    catchup=False,
) as dag:
    download = HttpToFilesystemOperator(
        task_id="download_daily_json",
        http_conn_id="my_api",
        endpoint="/v1/data",
        method="GET",
        target_path="/data/raw/api/daily.json",
    )
```

---

### 3.4 Ex√©cution dbt

```python
from datetime import datetime
from airflow import DAG
from airflow_toolkit.operators import DbtRunnerOperator

with DAG(
    dag_id="example_dbt_run",
    start_date=datetime(2025, 1, 1),
    schedule="@daily",
    catchup=False,
) as dag:
    dbt_run = DbtRunnerOperator(
        task_id="dbt_run_models",
        project_dir="/opt/dbt/project",
        profiles_dir="/opt/dbt/profiles",
        commands=["run", "--select", "tag:daily"],
    )
```

---

### 3.5 Ex√©cution SQL

```python
from datetime import datetime
from airflow import DAG
from airflow_toolkit.operators import SqlExecuteOperator

with DAG(
    dag_id="example_sql_maintenance",
    start_date=datetime(2025, 1, 1),
    schedule="@daily",
    catchup=False,
) as dag:
    vacuum = SqlExecuteOperator(
        task_id="vacuum_analyze",
        conn_id="postgres_admin",
        sql="VACUUM (VERBOSE, ANALYZE);",
    )
```

---

## 4. Utilitaires

### 4.1 Logger standardis√©

```python
from airflow_toolkit.utils import get_logger

logger = get_logger(__name__)
logger.info("Traitement en cours‚Ä¶")
```

---

### 4.2 Dates utilitaires

```python
from airflow_toolkit.utils import utc_now, days_ago

now = utc_now()
three_days = days_ago(3)
```

---

### 4.3 Chargement YAML par environnement

```python
from airflow_toolkit.utils import load_env_config

config = load_env_config(env="dev")
db_url = config["database"]["url"]
```

---

## 5. D√©veloppement local

### 5.1 Setup

```bash
git clone https://github.com/IADJALILProject/airflow-toolkit.git
cd airflow-toolkit

python -m venv .venv
source .venv/bin/activate

pip install -e ".[dev]"
```

---

### 5.2 Commandes locales

```bash
ruff check .
black .
pytest
```

---

## 6. Int√©gration Continue (CI)

Le workflow `.github/workflows/ci.yml` ex√©cute automatiquement :

- `ruff check .`
- `black --check .`
- `pytest`

---

## 7. Versioning & Releases

- Version actuelle : **1.0.0**
- Versioning s√©mantique : `MAJOR.MINOR.PATCH`
- Historique des changements : `CHANGELOG.md`

---

## 8. Auteur

**Abdeldjalil Salah-Bey**  
